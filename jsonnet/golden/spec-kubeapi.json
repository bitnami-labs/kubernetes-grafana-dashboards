{
   "grafana": {
      "common": {
         "extra": {
            "legend": {
               "rightSide": true
            }
         }
      },
      "templates_custom": {
         "api_percentile": {
            "default": "90",
            "hide": "",
            "values": "50, 90, 99"
         },
         "availability_span": {
            "default": "7d",
            "hide": "",
            "values": "10m,1h,1d,7d,30d,90d"
         },
         "verb_excl": {
            "default": "CONNECT|WATCH|PROXY",
            "hide": "variable",
            "values": "CONNECT|WATCH|PROXY"
         }
      }
   },
   "metrics": {
      "kube_api": {
         "alerts": {
            "blackbox": {
               "annotations": {
                  "description": "Issue: Kube API is not responding 200s from blackbox.monitoring\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIUnHealthy\n",
                  "summary": "Kube API is unhealthy"
               },
               "expr": "probe_success{provider=\"kubernetes\"} == 0",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPIUnHealthy"
            },
            "error_ratio": {
               "annotations": {
                  "description": "Issue: Kube API Error ratio on {{ $labels.instance }} is above 0.01: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIErrorRatioHigh\n",
                  "summary": "Kube API 500s ratio is High"
               },
               "expr": "sum by (instance)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m{verb=~\"GET|POST|DELETE|PATCH\", code=~\"5..\"}) > 0.01",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPIErrorRatioHigh"
            },
            "latency": {
               "annotations": {
                  "description": "Issue: Kube API Latency on {{ $labels.instance }} is above 200 ms: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPILatencyHigh\n",
                  "summary": "Kube API Latency is High"
               },
               "expr": "max by (instance)(kubernetes:job_verb_instance:apiserver_latency:pctl90rate5m{verb=~\"GET|POST|DELETE|PATCH\"}) > 200",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPILatencyHigh"
            }
         },
         "graphs": {
            "aa_availability_1": {
               "extra": {
                  "format": "percentunit",
                  "legend": {
                     "rightSide": false
                  },
                  "span": 2
               },
               "formula": "sum_over_time(kubernetes::job:slo_kube_api_ok[$availability_span]) / sum_over_time(kubernetes::job:slo_kube_api_sample[$availability_span])\n",
               "legend": "{{ job }}",
               "threshold": "0.99",
               "title": "SLO: Availaibility over $availability_span",
               "type": "singlestat"
            },
            "ab_availability_2": {
               "extra": {
                  "span": 4
               },
               "formula": "sum_over_time(kubernetes::job:slo_kube_api_ok[10m]) / sum_over_time(kubernetes::job:slo_kube_api_sample[10m])\n",
               "legend": "{{ job }}",
               "legend_rightSide": false,
               "threshold": "0.99",
               "title": "SLO: Availaibility over 10m"
            },
            "ac_error_ratio": {
               "extra": {
                  "span": 6
               },
               "formula": "sum by (job, verb, code, instance)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m{verb!~\"CONNECT|WATCH|PROXY\", code=~\"5..\"})",
               "legend": "{{ verb }} - {{ code }} - {{ instance }}",
               "threshold": 0.01,
               "title": "API Error ratio 500s/total (except CONNECT|WATCH|PROXY)"
            },
            "ba_req_ratio": {
               "extra": {
                  "legend": {
                     "rightSide": true
                  }
               },
               "formula": "kubernetes::job_verb_code:apiserver_requests:ratio_rate5m",
               "legend": "{{ verb }} - {{ code }}",
               "threshold": 1000000000,
               "title": "API requests ratios"
            },
            "ca_latency": {
               "extra": {
                  "legend": {
                     "rightSide": true
                  }
               },
               "formula": "kubernetes:job_verb_instance:apiserver_latency:pctl90rate5m{verb!~\"CONNECT|WATCH|PROXY\"}",
               "legend": "{{ verb }} - {{ instance }}",
               "threshold": 200,
               "title": "API $api_percentile-th latency[ms] by verb (except CONNECT|WATCH|PROXY)"
            }
         },
         "rules": {
            "latency_job_verb": {
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, verb)(\n    rate(apiserver_request_latencies_bucket[5m])\n  )\n) / 1e3 > 0\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes:job_verb:apiserver_latency:pctl90rate5m"
            },
            "latency_job_verb_instance": {
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, job, verb, instance)(\n    rate(apiserver_request_latencies_bucket[5m])\n  )\n) / 1e3\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes:job_verb_instance:apiserver_latency:pctl90rate5m"
            },
            "probe_success": {
               "expr": "sum by()(probe_success{provider=\"kubernetes\", component=\"apiserver\"})\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:probe_success"
            },
            "requests_rate_job_verb_code": {
               "expr": "sum without (instance)(kubernetes:job_verb_code_instance:apiserver_requests:rate5m)",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job_verb_code:apiserver_requests:rate5m"
            },
            "requests_rate_job_verb_code_instance": {
               "expr": "sum by (job, verb, code, instance)(rate(apiserver_request_count[5m]))",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes:job_verb_code_instance:apiserver_requests:rate5m"
            },
            "requests_ratiorate_job_verb_code": {
               "expr": "sum without (instance)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m)",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job_verb_code:apiserver_requests:ratio_rate5m"
            },
            "requests_ratiorate_job_verb_code_instance": {
               "expr": "kubernetes:job_verb_code_instance:apiserver_requests:rate5m / ignoring(verb, code) group_left sum by (job, instance)(kubernetes:job_verb_code_instance:apiserver_requests:rate5m)",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m"
            },
            "slo_errors_ratiorate_job": {
               "expr": "sum by (job)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m{verb=~\"GET|POST|DELETE|PATCH\", code=~\"5..\"})",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes:job:apiserver_request_errors:ratio_rate5m"
            },
            "slo_latency_job": {
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, job)(\n    rate(apiserver_request_latencies_bucket{verb=~\"GET|POST|DELETE|PATCH\"}[5m])\n  )\n) / 1e3\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:apiserver_latency:pctl90rate5m"
            },
            "slo_ok": {
               "expr": "kubernetes:job:apiserver_request_errors:ratio_rate5m < bool 0.01 * kubernetes::job:apiserver_latency:pctl90rate5m < bool 200\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:slo_kube_api_ok"
            },
            "slo_sample": {
               "expr": "kubernetes:job:apiserver_request_errors:ratio_rate5m < bool Inf * kubernetes::job:apiserver_latency:pctl90rate5m < bool Inf\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:slo_kube_api_sample"
            }
         }
      },
      "kube_control_mgr": {
         "alerts": {
            "work_duration": {
               "annotations": {
                  "description": "Issue: Kube Control Manager on {{ $labels.instance }} work duration is above 100: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeControllerWorkDurationHigh\n",
                  "summary": "Kube Control Manager workqueue processing is slow"
               },
               "expr": "sum by (instance)(\n  APIServiceRegistrationController_work_duration{quantile=\"0.9\"}\n) > 100\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeControllerWorkDurationHigh"
            }
         },
         "graphs": {
            "work_duration": {
               "extra": {
                  "legend": {
                     "rightSide": true
                  }
               },
               "formula": "sum by (instance)(\n  APIServiceRegistrationController_work_duration{quantile=\"0.9\"}\n)\n",
               "legend": "{{ instance }}",
               "threshold": 100,
               "title": "Kube Control Manager work duration"
            }
         },
         "name": "Kube Control Manager",
         "rules": { },
         "work_duration_limit": 100
      },
      "kube_etcd": {
         "alerts": {
            "latency": {
               "annotations": {
                  "description": "Issue: Kube Etcd latency on {{ $labels.instance }} above 1000 ms: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeEtcdLatencyHigh\n",
                  "summary": "Etcd Latency is High"
               },
               "expr": "max by (instance)(\n  etcd_request_latencies_summary{job=\"kubernetes_apiservers\",quantile=\"0.9\"}\n)/ 1e3 > 1000\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeEtcdLatencyHigh"
            }
         },
         "etcd_latency_threshold": 1000,
         "graphs": {
            "latency": {
               "extra": {
                  "legend": {
                     "rightSide": true
                  }
               },
               "formula": "max by (operation, instance)(\n  etcd_request_latencies_summary{job=\"kubernetes_apiservers\",quantile=\"0.9\"}\n)/ 1e3\n",
               "legend": "{{ instance }} - {{ operation }}",
               "threshold": 1000,
               "title": "etcd 90th latency[ms] by (operation, instance)"
            }
         },
         "name": "Kube Etcd",
         "rules": { }
      }
   },
   "prometheus": {
      "alerts_common": {
         "for": "5m",
         "labels": {
            "notify_to": "slack",
            "severity": "critical",
            "slack_channel": "#sre-alerts"
         }
      }
   }
}
