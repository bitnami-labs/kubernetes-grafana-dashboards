{
   "grafana": {
      "templates_custom": {
         "api_percentile": {
            "default": "90",
            "hide": "",
            "values": "50, 90, 99"
         },
         "verb_excl": {
            "default": "(CONNECT|WATCH)",
            "hide": "variable",
            "values": "(CONNECT|WATCH)"
         }
      }
   },
   "metrics": {
      "kube_api": {
         "alerts": {
            "blackbox": {
               "annotations": {
                  "description": "Issue: Kube API is not responding 200s from blackbox.monitoring\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIUnHealthy\n",
                  "summary": "Kube API is unhealthy"
               },
               "expr": "probe_success{provider=\"kubernetes\"} == 0\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPIUnHealthy"
            },
            "error_ratio": {
               "annotations": {
                  "description": "Issue: Kube API Error ratio on {{ $labels.instance }} is above 0.01: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIErrorRatioHigh\n",
                  "summary": "Kube API 500s ratio is High"
               },
               "expr": "sum by (instance)(\n  rate(apiserver_request_count{verb!~\"(CONNECT|WATCH)\", code=~\"5..\"}[5m])\n) /\nsum by (instance)(\n  rate(apiserver_request_count[5m])\n) > 0.01\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPIErrorRatioHigh"
            },
            "latency": {
               "annotations": {
                  "description": "Issue: Kube API Latency on {{ $labels.instance }} is above 200: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPILatencyHigh\n",
                  "summary": "Kube API Latency is High"
               },
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, instance)(\n    rate(apiserver_request_latencies_bucket{verb!~\"(CONNECT|WATCH)\"}[5m])\n  )\n) / 1e3 > 200\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPILatencyHigh"
            }
         },
         "api_percentile": "90",
         "error_ratio_threshold": 0.01,
         "graphs": {
            "error_ratio": {
               "formula": "sum by (verb, code)(\n  rate(apiserver_request_count{verb!~\"$verb_excl\", code=~\"5..\"}[5m])\n) / ignoring(code) group_left\nsum by (verb)(\n  rate(apiserver_request_count[5m])\n)\n",
               "legend": "{{ verb }} - {{ code }}",
               "threshold": 0.01,
               "title": "API Error ratio 500s/total (except $verb_excl)"
            },
            "latency": {
               "formula": "histogram_quantile (\n  0.$api_percentile,\n  sum by (le, verb)(\n    rate(apiserver_request_latencies_bucket{verb!~\"$verb_excl\"}[5m])\n  )\n) / 1e3 > 0\n",
               "legend": "{{ verb }}",
               "threshold": 200,
               "title": "API $api_percentile-th latency[ms] by verb (except $verb_excl)"
            }
         },
         "latency_threshold": 200,
         "name": "Kube API",
         "verb_excl": "(CONNECT|WATCH)"
      },
      "kube_control_mgr": {
         "alerts": {
            "work_duration": {
               "annotations": {
                  "description": "Issue: Kube Control Manager on {{ $labels.instance }} work duration is above 100: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeControllerWorkDurationHigh\n",
                  "summary": "Kube Control Manager workqueue processing is slow"
               },
               "expr": "sum by (instance)(\n  APIServiceRegistrationController_work_duration{quantile=\"0.9\"}\n) > 100\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeControllerWorkDurationHigh"
            }
         },
         "graphs": {
            "work_duration": {
               "formula": "sum by (instance)(\n  APIServiceRegistrationController_work_duration{quantile=\"0.9\"}\n)\n",
               "legend": "{{ instance }}",
               "threshold": 100,
               "title": "Kube Control Manager work duration"
            }
         },
         "name": "Kube Control Manager",
         "work_duration_limit": 100
      },
      "kube_etcd": {
         "alerts": {
            "latency": {
               "annotations": {
                  "description": "Issue: Kube Etcd latency on {{ $labels.instance }} above 20: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeEtcdLatencyHigh\n",
                  "summary": "Etcd Latency is High"
               },
               "expr": "max by (instance)(\n  rate(etcd_request_latencies_summary{job=\"kubernetes_apiservers\",quantile=\"0.9\"}[5m]) < Inf\n)/ 1e3 > 20\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeEtcdLatencyHigh"
            }
         },
         "etcd_latency_threshold": 20,
         "graphs": {
            "latency": {
               "formula": "max by (operation, instance)(\n  rate(etcd_request_latencies_summary{job=\"kubernetes_apiservers\",quantile=\"0.9\"}[5m]) < Inf\n)/ 1e3\n",
               "legend": "{{ instance }} - {{ operation }}",
               "threshold": 20,
               "title": "etcd 90th latency[ms] by (operation, instance)"
            }
         },
         "name": "Kube Etcd"
      }
   },
   "prometheus": {
      "alerts_common": {
         "for": "5m",
         "labels": {
            "notify_to": "slack",
            "severity": "critical",
            "slack_channel": "#sre-alerts"
         }
      }
   }
}
